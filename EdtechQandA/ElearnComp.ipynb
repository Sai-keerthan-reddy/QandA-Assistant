{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e1822be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_google_genai in c:\\users\\saike\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.45 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain_google_genai) (0.1.52)\n",
      "Requirement already satisfied: google-generativeai<0.6.0,>=0.5.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain_google_genai) (0.5.4)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.29.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.64.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.25.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.6.4)\n",
      "Requirement already satisfied: pydantic in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.7.1)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.19.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.11.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.129.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.23.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.45->langchain_google_genai) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.45->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.45->langchain_google_genai) (0.1.58)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.45->langchain_google_genai) (6.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langchain-core<0.3,>=0.1.45->langchain_google_genai) (23.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (5.3.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain_google_genai) (2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (2.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (3.10.3)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.63.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.1.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.22.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saike\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.4.5)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.62.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saike\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain_google_genai) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "#previously we used to import 'GooglePalm' from langchain.llms\n",
    "#but now we can directly import you can find it below \n",
    "\n",
    "\n",
    "!pip install langchain_google_genai  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2865993b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GoogleGenerativeAI in module langchain_google_genai.llms:\n",
      "\n",
      "class GoogleGenerativeAI(_BaseGoogleGenerativeAI, langchain_core.language_models.llms.BaseLLM)\n",
      " |  GoogleGenerativeAI(*, name: Optional[str] = None, cache: ForwardRef('Union[BaseCache, bool, None]') = None, verbose: bool = None, callbacks: ForwardRef('Callbacks') = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], List[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, model: str, google_api_key: Optional[pydantic.v1.types.SecretStr] = None, credentials: Any = None, temperature: float = 0.7, top_p: Optional[float] = None, top_k: Optional[int] = None, max_output_tokens: Optional[int] = None, n: int = 1, max_retries: int = 6, timeout: Optional[float] = None, client_options: Optional[Dict] = None, transport: Optional[str] = None, additional_headers: Optional[Dict[str, str]] = None, safety_settings: Optional[Dict[google.ai.generativelanguage_v1beta.types.safety.HarmCategory, google.ai.generativelanguage_v1beta.types.safety.SafetySetting.HarmBlockThreshold]] = None, client: Any = None) -> None\n",
      " |  \n",
      " |  Google GenerativeAI models.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_google_genai import GoogleGenerativeAI\n",
      " |          llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GoogleGenerativeAI\n",
      " |      _BaseGoogleGenerativeAI\n",
      " |      langchain_core.language_models.llms.BaseLLM\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Validates params and passes them to google-generativeai package.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'client': 'Any'}\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True, 'cu...\n",
      " |  \n",
      " |  __fields__ = {'additional_headers': ModelField(name='additional_header...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseLLM.raise_deprecatio...\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...kTh...\n",
      " |  \n",
      " |  __validators__ = {'verbose': [<pydantic.v1.class_validators.Validator ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseGoogleGenerativeAI:\n",
      " |  \n",
      " |  lc_secrets\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  __call__(self, prompt: 'str', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] Check Cache and run the LLM on the given prompt and input.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Get a string representation of the object for printing.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  async agenerate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, List[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of string prompts.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[str]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[LanguageModelInput]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, prompts: 'List[str]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, *, tags: 'Optional[Union[List[str], List[List[str]]]]' = None, metadata: 'Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]' = None, run_name: 'Optional[Union[str, List[str]]]' = None, run_id: 'Optional[Union[uuid.UUID, List[Optional[uuid.UUID]]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of string prompts.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Optional[Union[Callbacks, List[Callbacks]]]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-core==0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      " |      Save the LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the LLM to.\n",
      " |      \n",
      " |      Example:\n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          llm.save(file_path=\"path/llm.yaml\")\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[str]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.llms.BaseLLM:\n",
      " |  \n",
      " |  Config = <class 'langchain_core.language_models.llms.BaseLLM.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Get the number of tokens in the messages.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: The message inputs to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The sum of the number of tokens across the messages.\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Return the ordered ids of the tokens in a text.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
      " |              in the text.\n",
      " |  \n",
      " |  with_structured_output(self, schema: 'Union[Dict, Type[BaseModel]]', **kwargs: 'Any') -> 'Runnable[LanguageModelInput, Union[Dict, BaseModel]]'\n",
      " |      Implement this if there is a way of steering the model to generate responses that match a given schema.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
      " |      If verbose is None, set it.\n",
      " |      \n",
      " |      This allows users to pass in None as verbose to access the global setting.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for runnables that can be set at runtime.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |      \n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |      \n",
      " |          # uses ChatOpenaAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular runnable fields at runtime.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  is_lc_serializable() -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |      \n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |      \n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |      \n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |      \n",
      " |          print(chain_with_assign.input_schema.schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.schema()) #\n",
      " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Useful when a runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous runnable or included in the user input.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |      \n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |      \n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |      \n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |      \n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      \n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |      \n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |      \n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Any\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |      \n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |      \n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |      \n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |      \n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Iterator\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |      \n",
      " |      \n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |      \n",
      " |      \n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |      \n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |      \n",
      " |          def fn_start(run_obj : Runnable):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |      \n",
      " |          def fn_end(run_obj : Runnable):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |      \n",
      " |          RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |              ).invoke(2)\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          count = 0\n",
      " |      \n",
      " |      \n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |      \n",
      " |          assert (count == 2)\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if we dont have any idea , how its use, or have any doubts \n",
    "#simple go for help\n",
    "\n",
    "help(langchain_google_genai.GoogleGenerativeAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05de8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of importing from the langchain we can directly import the google genai langchain model\n",
    "#(before) from langchain.llm import GooglePalm\n",
    "import langchain_google_genai\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "api_key = \"AIzaSyC6t0ravji-aYztumaLWzTB9jzDEPNbEdk\"\n",
    "llm = GoogleGenerativeAI(google_api_key=api_key, temperature=0.7,model=\"gemini-pro\") \n",
    "#model='gemini-pro' is the new way where we pass model parameter\n",
    "#but in the previous version we dont need to give any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "132d7237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#below are old and new way of interacting with the LLM \n",
    "# Old (deprecated):\n",
    "#response = llm.__call__(prompt)  # This will trigger the warning\n",
    "\n",
    "# Updated (recommended):\n",
    "#response = llm.invoke(prompt)\n",
    "\n",
    "\n",
    "poem = llm.invoke(\"Write a 4 line poem on my love to biryani\") \n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a89a75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to import a csv file we need to import CSVLoader from the langchain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4f70123",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"codebasics_faqs.csv\",source_column=\"prompt\")\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1633731a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='prompt: I have never done programming in my life. Can I take this bootcamp?\\nresponse: Yes, this is the perfect bootcamp for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in your current job or business using data.', metadata={'source': 'I have never done programming in my life. Can I take this bootcamp?', 'row': 0}),\n",
       " Document(page_content='prompt: Why should I trust Codebasics?\\nresponse: Till now 9000 + learners have benefitted from the quality of our courses. You can check the review section and also we have attached their LinkedIn profiles so that you can connect with them and ask directly.', metadata={'source': 'Why should I trust Codebasics?', 'row': 1}),\n",
       " Document(page_content='prompt: Is there any prerequisite for taking this bootcamp ?\\nresponse: Our bootcamp is specifically designed for beginners with no prior experience in this field. The only prerequisite is that you need to have a functional laptop with at least 4GB ram, an internet connection, and a thrill to learn data analysis.', metadata={'source': 'Is there any prerequisite for taking this bootcamp ?', 'row': 2}),\n",
       " Document(page_content='prompt: What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?\\nresponse: The datasets used in this bootcamp are crafted from scratch to mimic the real business world by consolidating our years of experience. The datasets not just contain more than a million rows but also cover multiple facets of the business.', metadata={'source': 'What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?', 'row': 3}),\n",
       " Document(page_content='prompt: I’m not sure if this bootcamp is good enough for me to invest some \\nmoney. What can I do?\\nresponse: We got you covered. Go ahead and watch our youtube videos if you like them and want to learn further then this bootcamp is the perfect extension.', metadata={'source': 'I’m not sure if this bootcamp is good enough for me to invest some \\nmoney. What can I do?', 'row': 4}),\n",
       " Document(page_content='prompt: How can I contact the instructors for any doubts/support?\\nresponse: We have created every lecture with a motive to explain everything in an easy-to-understand manner. While working on these lectures you could make mistakes in steps or have some doubts. You need to commit yourself to hold patience, make efforts & diagnose the errors yourself by googling in order to become truly job ready. For any questions, that Google cannot answer or if you hit a wall - we got you covered! You can join our active discord community. which is a dedicated platform to discuss & clear your doubts with fellow learners & mentors.', metadata={'source': 'How can I contact the instructors for any doubts/support?', 'row': 5}),\n",
       " Document(page_content='prompt: What if I don’t like this bootcamp?\\nresponse: As promised we will give you a 100% refund based on the guidelines (Please refer to our course refund policy before enrolling).', metadata={'source': 'What if I don’t like this bootcamp?', 'row': 6}),\n",
       " Document(page_content='prompt: Does this bootcamp have lifetime access?\\nresponse: Yes', metadata={'source': 'Does this bootcamp have lifetime access?', 'row': 7}),\n",
       " Document(page_content='prompt: What is the duration of this bootcamp? How long will it last?\\nresponse: You can complete all courses in 3 months if you dedicate 2-3 hours per day.', metadata={'source': 'What is the duration of this bootcamp? How long will it last?', 'row': 8}),\n",
       " Document(page_content='prompt: Can I attend this bootcamp while working full time?\\nresponse: Yes. This bootcamp is self-paced. You can learn on your own schedule.', metadata={'source': 'Can I attend this bootcamp while working full time?', 'row': 9}),\n",
       " Document(page_content='prompt: What kind of skills can I expect to learn in this bootcamp?\\nresponse: You can expect to learn tech skills like Python, Excel, SQL, and Power BI along with other skills like basic project management, stakeholder management, communication skills, and Business knowledge for domains like Finance, Supply chain, Sales, and Marketing.', metadata={'source': 'What kind of skills can I expect to learn in this bootcamp?', 'row': 10}),\n",
       " Document(page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.', metadata={'source': 'Do you provide any job assistance?', 'row': 11}),\n",
       " Document(page_content='prompt: Is this bootcamp enough for me in Microsoft Power BI and\\n Excel certifications?\\nresponse: Yes, this bootcamp will certainly help because we cover the majority of the skills measured in these exams. However, please be informed that this course focuses on Job ready aspects and not on all aspects required to clear the exams. In addition to this course, you might need to visit the official learning material designed by Microsoft which is available for free on their website.', metadata={'source': 'Is this bootcamp enough for me in Microsoft Power BI and\\n Excel certifications?', 'row': 12}),\n",
       " Document(page_content='prompt: Do we have an EMI option?\\nresponse: No', metadata={'source': 'Do we have an EMI option?', 'row': 13}),\n",
       " Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}),\n",
       " Document(page_content='prompt: Will this bootcamp guarantee me a job?\\nresponse: The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this bootcamp guarantee me a job?', 'row': 15}),\n",
       " Document(page_content='prompt: I have zero knowledge of Excel and belong to a non-technical\\n background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never worked on excel and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.', metadata={'source': 'I have zero knowledge of Excel and belong to a non-technical\\n background. Can I take this course?', 'row': 16}),\n",
       " Document(page_content='prompt: What are the things I need to know before starting this course?\\nresponse: This course is for absolute beginners hence you do not need any specific skills other than basic familiarity with computers', metadata={'source': 'What are the things I need to know before starting this course?', 'row': 17}),\n",
       " Document(page_content='prompt: What is different in this course compared to hundreds of courses on the internet and free tutorials on YouTube?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for real business world problem-solving. This course is rather an experience in which you will learn Excel by solving real-life use cases in an imaginary company called AtliQ Hardware. The tutorials are very easy to understand and also have a lot of fun elements into them so that you don’t get bored ??', metadata={'source': 'What is different in this course compared to hundreds of courses on the internet and free tutorials on YouTube?', 'row': 18}),\n",
       " Document(page_content='prompt: Can I add this course to my resume?\\nresponse: Yes. Absolutely you can mention the AtliQ Hardware project experience in your resume with the relevant skills that you will learn from this course', metadata={'source': 'Can I add this course to my resume?', 'row': 19}),\n",
       " Document(page_content='prompt: I’m not sure if this course is good enough for me to invest some money. What can I do?\\nresponse: Don’t worry. Many videos in this course are free so watch them to get an idea of the quality of teaching. Dhaval Patel (the course instructor) runs a popular data science YouTube channel called Codebasics. On that, you can watch his videos and read comments to get an idea of his teaching style', metadata={'source': 'I’m not sure if this course is good enough for me to invest some money. What can I do?', 'row': 20}),\n",
       " Document(page_content='prompt: What dataset is used in this course? Is it some toy dataset or something that mimics a real-world problem?\\nresponse: For initial basics, we have used a toy dataset of movies but for advanced concepts, we used a dataset that is crafted to mimic the real business world by consolidating our years of experience and the FMCG industry. That dataset contains more than 1 million records.', metadata={'source': 'What dataset is used in this course? Is it some toy dataset or something that mimics a real-world problem?', 'row': 21}),\n",
       " Document(page_content='prompt: Once purchased, is this course available for lifetime access?\\nresponse: Yes', metadata={'source': 'Once purchased, is this course available for lifetime access?', 'row': 22}),\n",
       " Document(page_content='prompt: How can I get help if I have a doubt and need support?\\nresponse: We have an active discord server where you can post your question and most of the time you will get an answer in a reasonable time frame.', metadata={'source': 'How can I get help if I have a doubt and need support?', 'row': 23}),\n",
       " Document(page_content='prompt: I have never done programming and belong to a non-technical background. Can I take this course?\\nresponse: Yes, this is the perfect course for anyone who has never done coding and wants to build a career in the IT/Data Analytics industry or just wants to perform better in their current job or business using data.', metadata={'source': 'I have never done programming and belong to a non-technical background. Can I take this course?', 'row': 24}),\n",
       " Document(page_content='prompt: I don’t have a laptop, can I take this course?\\nresponse: We recommend learning by doing and therefore you need to have a laptop or a PC (at least 4 GB ram).', metadata={'source': '\\nI don’t have a laptop, can I take this course?', 'row': 25}),\n",
       " Document(page_content='prompt: Will the course help me in PL - 300 Microsoft exam preparation?\\nresponse: Yes, this course will certainly help because we cover the majority of the skills measured in the PL - 300 exam in this course. However, please be informed that this course focuses on Job ready aspects and not on all aspects required to clear PL - 300 exam. In addition to this course, you might need to visit the official learning material designed by Microsoft which is available for free -> https://docs.microsoft.com/en-us/certifications/exams/pl-300?tab=tab-learning-paths', metadata={'source': 'Will the course help me in PL - 300 Microsoft exam preparation?', 'row': 26}),\n",
       " Document(page_content='prompt: Will the course be upgraded when there are new features in Power BI?\\nresponse: Yes, the course will be upgraded periodically based on the new features in Power BI, and learners who have already bought this course will have free access to the upgrades.', metadata={'source': 'Will the course be upgraded when there are new features in Power BI?', 'row': 27}),\n",
       " Document(page_content='prompt: I use tableau, can I take this course?\\nresponse: Yes, you will still benefit from the concepts outside the tools that are discussed in this course such as business context, problem solving, project management tools, etc.', metadata={'source': 'I use tableau, can I take this course?', 'row': 28}),\n",
       " Document(page_content='prompt: Power BI or Tableau which one is better?\\nresponse: This is a contextual question. If you are talking about a pure visualization tool Tableau is slightly better. Data connectors, modeling and transformation features are available in both. However, factually speaking Power BI is cheaper and offers tighter integration with the Microsoft environment. Since most companies use excel & Microsoft tools they start with Power BI or move towards Power BI for seamless integration with other Microsoft tools (called as Power platform). This makes the job openings grow at a much higher rate on Power BI and Power Platform. Also, Power BI has been leading the Gartner’s magic quadrant in BI for the last few years as the industry leader.', metadata={'source': '\\nPower BI or Tableau which one is better?', 'row': 29}),\n",
       " Document(page_content='prompt: What dataset is used in this course? Is it some Toy dataset or something that mimics a real-world business problem?\\nresponse: The dataset used in this course is crafted from scratch to mimic the real business world by consolidating our years of experience. The dataset not just contains more than a million rows but also covers multiple facets of business data such as sales, finance, targets, forecasts, products, etc.', metadata={'source': 'What dataset is used in this course? Is it some Toy dataset or something that mimics a real-world business problem?', 'row': 30}),\n",
       " Document(page_content='prompt: Does Power BI work in Mac OS/Ubuntu?\\nresponse: Power BI desktop works only in Windows OS. Please look into the system requirements section on this page. However, you can use a virtual machine to install and work with Power BI in other Operating systems.', metadata={'source': 'Does Power BI work in Mac OS/Ubuntu?', 'row': 31}),\n",
       " Document(page_content='prompt: What business concepts and domains are covered in this course?\\nresponse: We have covered the core functions such as Sales, Marketing, Finance, and Supply Chain with their fundamentals related to this course. The domain you will learn in this course is consumer goods which is projected to have more openings and high data analytics requirements at least until 2030.', metadata={'source': 'What business concepts and domains are covered in this course?', 'row': 32}),\n",
       " Document(page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}),\n",
       " Document(page_content='prompt: Can I add this course to my resume? If Yes, how?\\nresponse: Absolutely, we have a section in this course explaining how you can add the learnings from this course in your resume that will appeal to the hiring team.', metadata={'source': 'Can I add this course to my resume? If Yes, how?', 'row': 34}),\n",
       " Document(page_content='prompt: Is there any prerequisite for taking this course?\\nresponse: The only prerequisite is that you need to have a functional laptop with at least 4GB ram, internet connection and a thrill to learn data analysis.', metadata={'source': 'Is there any prerequisite for taking this course?', 'row': 35}),\n",
       " Document(page_content='prompt: What is different in this course from thousands of other Power BI courses available online?\\nresponse: Most of the courses available on the internet teach you how to build x & y without any business context and do not prepare you for the real business world. This course is rather an experience in which you will learn how to use Power BI & other non-technical skills to solve a real-life business problem using analytics. Here you focus on solving a business problem and in that process learn how Power BI can be used as a tool. This is how you will do the work when you start working as a data analyst/ Business analyst/ Power BI developer in the industry. This course will prepare you for not just fetching the job but, shine in it & grow further.', metadata={'source': 'What is different in this course from thousands of other Power BI courses available online?', 'row': 36}),\n",
       " Document(page_content='prompt: I already know basic Power BI, what benefit do I get by taking this course?\\nresponse: This course is taught through a true end-to-end project in a Consumer goods company involving all the steps mimicking the real business environment, so you will learn how to execute end-to-end projects Power BI projects successfully along with the business fundamentals. You will learn a lot of extra things such as Project management tools, effective communication techniques & organizational nuances.', metadata={'source': 'I already know basic Power BI, what benefit do I get by taking this course?', 'row': 37}),\n",
       " Document(page_content='prompt: How to install power pivot if its not available in system?\\nresponse: Follow this thread for instructions - https://support.microsoft.com/en-us/office/start-the-power-pivot-add-in-for-excel-a891a66d-36e3-43fc-81e8-fc4798f39ea8\\nIf it doesn\\'t show in the ribbon then go to \"insert\" tab. You will be able to see pivot table option there.', metadata={'source': 'How to install power pivot if its not available in system?', 'row': 38}),\n",
       " Document(page_content='prompt: Hi all, could someone please help me understand what I am writing wrong in this code?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code)\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)', metadata={'source': 'Hi all, could someone please help me understand what I am writing wrong in this code?', 'row': 39}),\n",
       " Document(page_content='prompt: I am getting this error, what is wrong?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code) and also provide the error you are getting\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)', metadata={'source': 'I am getting this error, what is wrong?', 'row': 40}),\n",
       " Document(page_content=\"prompt: Hey all, I've been trying this for past one hour but not working. \\nI am getting this error in my SQL code, what could be wrong?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and type this command\\n\\nDebug me this code ( copy paste your code) and also provide the error you are getting\\n\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)\", metadata={'source': \"Hey all, I've been trying this for past one hour but not working. \\nI am getting this error in my SQL code, what could be wrong?\", 'row': 41}),\n",
       " Document(page_content='prompt: when I am entering the index match formula in the cell outside the table it is working and when i enter the same formula inside the table cell it is showing a SPILL! error ..why?\\nresponse: Go to ChatGPT (https://chat.openai.com/) and ask this same question.\\n\\nWhy I am suggesting this way is once you learn it you will love it. \\nAnd it will be SUPER USEFUL in your career when you get a data analyst jobs. Companies love folks who are capable of finding their answers on their own (Using Google, ChatGPT etc.)', metadata={'source': 'when I am entering the index match formula in the cell outside the table it is working and when i enter the same formula inside the table cell it is showing a SPILL! error ..why?', 'row': 42}),\n",
       " Document(page_content=\"prompt: I am not allowing to post doubt in the discord group\\nresponse: Sure I can guide you\\n\\nGo to the 'click-here-to-ask-questions' section and verify yourself here by clicking on the checkmark.\", metadata={'source': 'I am not allowing to post doubt in the discord group', 'row': 43}),\n",
       " Document(page_content='prompt: How can I use PowerBI on my Mac system?\\nresponse: Hi\\n\\nYou can use VirtualBox to create a virtual machine and install Windows on it. This will allow you to run Power BI and Excel on your Mac.\\n\\nIf you\\'re not familiar with setting up a virtual machine, there are many resources available on YouTube that can guide you through the process. Simply search for \"installing virtual machines\" and you\\'ll find plenty of helpful videos.\\n\\nBest of luck with your studies!', metadata={'source': 'How can I use PowerBI on my Mac system?', 'row': 44}),\n",
       " Document(page_content='prompt: In sales analytics chapter , when creating pivot table a blank column is getting added. How to solve this?\\nresponse: Hi, please check these instructions.\\n\\nOption - 1:\\nIf you are from India, you can change your system default date format to India, then restart your computer once. and see, whether it fixed your issue or not. Else you can follow the second option.\\n\\nOption - 2:\\n\\n1. Reason for the Issue:\\nIn the dim_date table, the date is in dd/mm/yyyy format. Whereas the fact table is in mm/dd/yyyy format. Because of the different formats, causing the issue. Power BI/Excel will likely use your system\\'s default date format whenever you open any file. This could cause the date format issue - dd/mm/yyyy --- mm/dd/yyyy. Please look for this whenever you face issues with values mismatch.\\n\\n2. Steps to resolve:\\n    1. Change the data type of the date column to ‘Date’ if it is in ‘Date/Time’ format. Skip this step if it is already in ‘Date’ format.\\n    2. Create a new custom column in the fact_sales_monthly table. you can name it date_modified or any other name accordingly.\\n    Formula\\n    = Date.ToText([date],\"yyyy\") & \"/\" & Text.PadStart(Text.From(Date.Day([date])),2,\"0\") & \"/\" & Text.PadStart(Text.From(Date.Month([date])),2,\"0\")\\n    3. change the data type of the above-created column to date.\\n    4. In Power Pivot, Go to the \\'Manage\\' tab and select the \\'Diagram\\' view. Map the date column in the \\'dim_date\\' table with the newly created column in the \\'fact_sales_monthly\\' table. Make sure to delete the previous relationship (date from fact_sales_month).', metadata={'source': 'In sales analytics chapter , when creating pivot table a blank column is getting added. How to solve this?', 'row': 45}),\n",
       " Document(page_content='prompt: why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?\\nresponse: You have selected Table Visual instead of Matrix. That is why you are seeing a different interface.', metadata={'source': 'why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?', 'row': 46}),\n",
       " Document(page_content='prompt: Hello, I have a issue to calculate WHF%. Can you give me numbers for checklist and debbuging?\\nresponse: Hello \\n\\nPlease confirm whether you got these values right!\\n\\nBefore Data Cleaning Stats:\\nNumber of rows: 6336\\nNumber of distinct employees: 198\\nNumber of distinct employee_id: 106\\n\\nAfter Data Cleaning Stats:\\nNumber of rows: 6208\\nNumber of distinct employees: 99\\nNumber of distinct employee_id: 74\\n\\nIf these values are not matching, I highly suggest doing the data-cleaning steps correctly (mainly removing duplicates)', metadata={'source': 'Hello, I have a issue to calculate WHF%. Can you give me numbers for checklist and debbuging?', 'row': 47}),\n",
       " Document(page_content=\"prompt: How do become good and comfortable with SQL?\\nresponse: To master SQL, we need to practice it every day and keep in touch with it all the time.\\n\\n---\\nHere are some references where you can practice daily. First start with easy problems, then medium, and finally hard questions. Don't directly jump into Hard questions.\\n- Hacker rank: https://www.hackerrank.com/domains/sql\\n- Leetcode: https://leetcode.com/problemset/database/\\n- Datalemur: https://datalemur.com/\\n---\\n\\nI have created a LinkedIn post attaching some youtube resources which discussed interview-related problems and help to solve interesting questions. check out this too.\\nLink:  https://www.linkedin.com/posts/kirandeepmarala_sql-careers-data-activity-7012258071334854656-jRgf?utm_source=share&utm_medium=member_desktop\\n---\\n\\nAlso, I highly recommend participating in the Codebasics SQL challenge, which has been conducted in the past, as it can help you brush up on your skills and provide an opportunity to test your knowledge further\\nLink:  https://codebasics.io/challenge/codebasics-resume-project-challenge  (Challenge #4)\", metadata={'source': 'How do become good and comfortable with SQL?', 'row': 48}),\n",
       " Document(page_content='prompt: Why SUM() function not working in my Excel?\\nresponse: Because of having  #N/A values, the SUM() function is not working as expected.\\n\\nso we need to handle this case by filling the #N/A(not available) with 0.\\n\\nThread for further explanation: https://discord.com/channels/1090613684163850280/1107909641989525524/1107926537988227073', metadata={'source': 'Why SUM() function not working in my Excel?', 'row': 49}),\n",
       " Document(page_content='prompt: What is the difference between Business Analyst Vs Data Analyst\\nresponse: While both business analysts and data analysts work with data, they have different focuses and responsibilities. Here are some key distinctions:\\n\\n1. Focus: Business analysts focus on the business side of things, such as business processes, workflows, and strategies. They use data analysis to identify business problems, opportunities, and solutions. Data analysts, on the other hand, focus on the data itself. They collect, process, and analyze data to extract insights and make data-driven decisions.\\n2. Skill set: Business analysts need to have strong communication, problem-solving, and strategic thinking skills, as they often work closely with stakeholders and decision-makers to identify business needs and develop solutions. They also need to have some knowledge of data analysis, such as data visualization and basic statistical analysis. Data analysts, on the other hand, need to have a strong foundation in data analysis and programming skills, such as SQL, Python, etc. They also need to have a good understanding of data visualization and statistical analysis to derive insights from the data.\\n3. Tools: Business analysts typically use tools like spreadsheets, business process modeling software, and business intelligence (BI) tools to analyze data and create reports. Data analysts use more advanced tools like statistical software, programming languages, and database management systems to collect, process, and analyze data.\\n4. Outputs: Business analysts typically produce reports, presentations, and recommendations based on their analysis of the data. Data analysts produce statistical models, visualizations, and reports to help stakeholders understand the data and make informed decisions.\\n\\nI recommend watching this video by Dhaval sir which explained more details: https://www.youtube.com/watch?v=7sFRpbPKTG4&t=1s', metadata={'source': 'What is the difference between Business Analyst Vs Data Analyst', 'row': 50}),\n",
       " Document(page_content=\"prompt: Can you state the distinction among the SAMEPERIODLASTYEAR(), PARALLELPERIOD(), and DATEADD() function in Power BI?\\nresponse: Here is the brief explanation.\\n\\nSAMEPERIODLASTYEAR() compares the given period with the exact same period of last year as the name suggests. But it has a limitation like King of the chess board - it can go only compare 1 year backward for a given day, month, week, or quarter. If you want to go backward or forward or if you need to go 3 or 6 months/years/days you need to use DATEADD().\\n\\nDATEADD() is more like the queen of the chessboard. It is very dynamic as it compares a day, month, quarter, year, etc. over any specified no. of time period.\\n\\nPARALLELPERIOD() works exactly like DATEADD() but there is a minor difference. Parallelperiod() does not work dynamically like DATEADD().\\n\\nFor example, if you use PARALLELPERIOD() to get the previous month's value for dates between 1/5/2023 - 12/5/2023..it will compare these 12 days with the entire previous month of April 2023 (1/4/2023 - 30/4/2023) whereas DATEADD will compare the exact same days (1/4/2023- 12/4/2023).\\n\\nLikewise, if you use PARALLELPERIOD() to get the previous year's value for dates between 1/1/2023 - 12/5/2023..it will compare this time period with the entire previous year 2022 (1/1/2022 - 31/12/2022) whereas DATEADD will compare the exact same period (1/1/2022- 12/5/2022).\\n\\nRefer this thread for more info: https://discord.com/channels/1090613684163850280/1106528524023648326\", metadata={'source': 'Can you state the distinction among the SAMEPERIODLASTYEAR(), PARALLELPERIOD(), and DATEADD() function in Power BI?', 'row': 51}),\n",
       " Document(page_content=\"prompt: My Excel Excel VLookup() showing formula instead of result\\nresponse: Solution1:\\n\\nPlease check the cell type, if it is 'text', change that to 'general', and once hit enter execute the formula.\\n\\nSolution2:\\n\\nThe primary reason for this issue is the accidental enabling of the 'show formulas' mode.\\n To gain more clarity and resolve the problem, please refer to the following resource: https://exceljet.net/articles/excel-shows-formula-not-result\", metadata={'source': 'My Excel Excel VLookup() showing formula instead of result', 'row': 52}),\n",
       " Document(page_content='prompt: I have a doubt regarding Vlookup() giving zero values in Exercise-3.\\nresponse: check this demo video to fix the issue: https://drive.google.com/file/d/1gp0krl2wMN0YhZYja3IKmt59SBsUK7xB/view?usp=share_link', metadata={'source': 'I have a doubt regarding Vlookup() giving zero values in Exercise-3.', 'row': 53}),\n",
       " Document(page_content='prompt: i am unable to import data from mysql in power bi ,connector issue is coming continuously i have done all steps according to connector pdf still its not resolving please guide\\nresponse: Please refer to this thread: https://discord.com/channels/1090613684163850280/1107992760105054238/1107993007606730802', metadata={'source': 'i am unable to import data from mysql in power bi ,connector issue is coming continuously i have done all steps according to connector pdf still its not resolving please guide', 'row': 54}),\n",
       " Document(page_content='prompt: Do data analysts need to learn database design?\\nresponse: As a data analyst, it is not typically expected that you would be responsible for creating a database from scratch. However, you may be involved in designing the structure and schema of a database, as well as configuring and populating it with data.\\n\\nCreating a database usually falls under the purview of a database administrator (DBA) or a data engineer. These roles are focused on the technical aspects of database design, implementation, and maintenance.\\n\\nAs a data analyst, your primary responsibilities would typically include querying and analyzing data within an existing database, creating reports and visualizations to communicate insights, and helping to inform business decisions based on data analysis. However, depending on the organization and the specific role, there may be some overlap or variation in responsibilities.', metadata={'source': 'Do data analysts need to learn database design?', 'row': 55}),\n",
       " Document(page_content='prompt: Can you explain me more about the foreign key?\\nresponse: A foreign key is a column or set of columns in a table that refers to the primary key of another table. It is used to establish a relationship between two tables in a relational database. The primary key of one table is used as a foreign key in another table to link the data between the tables.\\n\\nFor example, let\\'s say we have two tables in a database: a \"customers\" table and an \"orders\" table. The customer\\'s table has a primary key column called \"customer_id\", and the orders table has a foreign key column called \"customer_id\" that references the \"customer_id\" column in the customer\\'s table.\\n\\nBy establishing this relationship between the two tables, we can ensure that every order in the orders table is associated with a valid customer in the customer\\'s table.', metadata={'source': 'Can you explain me more about the foreign key?', 'row': 56}),\n",
       " Document(page_content='prompt: I\\'m experiencing an issue where the target value appears the same in every row, and it is incorrect. Can you please help me understand why this is happening?\\nresponse: That is happening because you are pulling \"market\" from wrong table\\nPull \"Market\" From dim_market\\n\\nCheck out this thread : https://discord.com/channels/1090613684163850280/1108671672434839562/1108672494958817301', metadata={'source': \"I'm experiencing an issue where the target value appears the same in every row, and it is incorrect. Can you please help me understand why this is happening?\", 'row': 57}),\n",
       " Document(page_content='prompt: Hello Everyone, I am using 2016 excel. So far, I have noticed there are few functions which are not available in 2016 Excel like xlookup, unique etc. is there any option \\navailable other than buying new updated version ?\\nresponse: Here are some add-in links. Please go through the videos , you will be able to use those functions in your Excel version. \\n\\n? UNIQUE function:\\n?? https://youtu.be/wiCH-YTacKU\\n\\n?XLOOKUP function: \\n?? https://www.youtube.com/watch?v=NWZaM7ZbE3Q\\nor,\\n?? https://www.youtube.com/watch?v=e60-J_u-K7o\\n\\n?? https://www.youtube.com/watch?v=TUTMfhCS62M', metadata={'source': 'Hello Everyone, I am using 2016 excel. So far, I have noticed there are few functions which are not available in 2016 Excel like xlookup, unique etc. is there any option \\navailable other than buying new updated version ?', 'row': 58}),\n",
       " Document(page_content='prompt: Can someone help me to create same chart exactly like this. I have created more than half of it. I want to add quadrant labels. Low medium high on x & y axis\\nresponse: Check this thread for instructions : https://discord.com/channels/1090613684163850280/1107745807848980490/1107746891418058762', metadata={'source': 'Can someone help me to create same chart exactly like this. I have created more than half of it. I want to add quadrant labels. Low medium high on x & y axis', 'row': 59}),\n",
       " Document(page_content=\"prompt: whenever I am loading fact_sales_monthly table in Power query not all columns are visible\\nresponse: Have you taken SQL course in Codebasics first ?\\nIf yes, delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\\n\\nCheck this reference too: https://discordapp.com/channels/1090613684163850280/1111180401478746163/1111190746507251732\", metadata={'source': 'whenever I am loading fact_sales_monthly table in Power query not all columns are visible', 'row': 60}),\n",
       " Document(page_content=\"prompt: The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?\\nresponse: Delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\", metadata={'source': 'The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?', 'row': 61}),\n",
       " Document(page_content='prompt: I am getting this error in PowerBI, \\n\\n\"there are pending changes in your queries that have havent been applied\"\\n\\nAny suggestions to solve it?\\nresponse: To resolve the annoying message, follow these steps:\\n1. Access Power Query by selecting \\'Transform Data.\\'\\n2. Click on \\'Refresh Preview\\' and then choose \\'Refresh All\\' to run a refresh on the tables.\\n3. After completing the refresh, select \\'Close & Apply\\' to save the changes.\\nBy following these steps, the annoying message should disappear.\\n\\nCheck this reference for further details: https://community.fabric.microsoft.com/t5/Desktop/quot-There-are-pending-changes-in-your-queries-that-haven-t-been/td-p/1142424\\nHope this helps!', metadata={'source': 'I am getting this error in PowerBI, \\n\\n\"there are pending changes in your queries that have havent been applied\"\\n\\nAny suggestions to solve it?', 'row': 62}),\n",
       " Document(page_content='prompt: I am encountering an error with the pre-invoice discount where I am receiving \"0\" values. Could you assist me in resolving this issue?\\nresponse: To troubleshoot the issue, please consider the following steps:\\n\\n1. Verify your data modeling by accessing \"Manage Relationships.\"\\n2. Review the measures you have created for any potential errors or inconsistencies.\\n3. Ensure that the column format is set correctly as the \"Fixed Decimal Number\" type.\\nBy examining these aspects, you can address the pre-invoice discount error and resolve the issue.\\n\\nCheck this reference for further details: https://discordapp.com/channels/1090613684163850280/1106513101693669466/1106516289905766433', metadata={'source': 'I am encountering an error with the pre-invoice discount where I am receiving \"0\" values. Could you assist me in resolving this issue?', 'row': 63}),\n",
       " Document(page_content='prompt: The year column is missing in the P&L check. How can I resolve this issue and obtain the year column?\\nresponse: Check this reference:\\n https://discord.com/channels/1090613684163850280/1111101322406658098/1111137901816848494', metadata={'source': 'The year column is missing in the P&L check. How can I resolve this issue and obtain the year column?', 'row': 64}),\n",
       " Document(page_content='prompt: There are missing values in the P&L check. How can I address this issue and handle the missing values appropriately?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1108980495477395486/1109016973662240848', metadata={'source': 'There are missing values in the P&L check. How can I address this issue and handle the missing values appropriately?', 'row': 65}),\n",
       " Document(page_content='prompt: How can I rearrange the column order in the P&L check? I would like to change the order of the columns.\\nresponse: Check this reference:\\nhttps://discord.com/channels/1090613684163850280/1112342820930453516/1112399292792062022', metadata={'source': 'How can I rearrange the column order in the P&L check? I would like to change the order of the columns.', 'row': 66}),\n",
       " Document(page_content='prompt: Why is the year 2018 missing or disappeared?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1111545547426369637/1111563527753318430', metadata={'source': 'Why is the year 2018 missing or disappeared?', 'row': 67}),\n",
       " Document(page_content=\"prompt: I'm experiencing a flat line in the Line chart of the Finance View. How can I address this issue and ensure the chart displays the expected data correctly?\\nresponse: Check this reference:\\nhttps://discord.com/channels/1090613684163850280/1112243985784774687/1112244353126113302\", metadata={'source': \"I'm experiencing a flat line in the Line chart of the Finance View. How can I address this issue and ensure the chart displays the expected data correctly?\", 'row': 68}),\n",
       " Document(page_content='prompt: During the process of creating the line chart for forecast accuracy vs month in the Supply Chain View, I encountered an issue with the visual representation.\\nThe problem occurred at the 5:00-minute mark. I selected the year 2021, expecting the graph to display data only from September 2020 to August 2021.\\nHowever, instead of the desired timeframe, the chart displayed data from all months between 2017 and 2022.\\n\\nHow to solve and fix this issue?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1113690341929918505/1113750218618445824', metadata={'source': 'During the process of creating the line chart for forecast accuracy vs month in the Supply Chain View, I encountered an issue with the visual representation.\\nThe problem occurred at the 5:00-minute mark. I selected the year 2021, expecting the graph to display data only from September 2020 to August 2021.\\nHowever, instead of the desired timeframe, the chart displayed data from all months between 2017 and 2022.\\n\\nHow to solve and fix this issue?\\n', 'row': 69}),\n",
       " Document(page_content='prompt: It appears that the X-axis of the chart is not starting with the month of September as expected. Instead, it is starting with a different month.\\nHow to solve this?\\nresponse: Check this reference:\\nhttps://discordapp.com/channels/1090613684163850280/1112567189724213339/1112596364862423090', metadata={'source': 'It appears that the X-axis of the chart is not starting with the month of September as expected. Instead, it is starting with a different month.\\nHow to solve this?', 'row': 70}),\n",
       " Document(page_content='prompt: Why we use Net error in place of absolute net error\\nresponse: Directional Insight: The net error metric offers valuable information regarding the direction of the error in forecasting, distinguishing between positive (overestimation) and negative (underestimation) values. This enables the identification of specific products that consistently outperform or underperform relative to the expected values. Additionally, net error provides insights into the risk factor, indicating if there are instances of stockouts or excessive inventory levels.', metadata={'source': 'Why we use Net error in place of absolute net error', 'row': 71}),\n",
       " Document(page_content=\"prompt: I am encountering an issue where the NIS IND column has the same value in all rows. As a result, I am unable to obtain the correct value for the benchmark variable.\\n How can I address this problem and obtain accurate benchmark values?\\nresponse: Have you taken the 'market' column from dim_market? Can you please check again. \\n\\nCheck this reference for further details: https://discordapp.com/channels/1090613684163850280/1107202294476443789/1107208944931311647\", metadata={'source': 'I am encountering an issue where the NIS IND column has the same value in all rows. As a result, I am unable to obtain the correct value for the benchmark variable.\\n How can I address this problem and obtain accurate benchmark values?', 'row': 72}),\n",
       " Document(page_content='prompt: How do I update source in power query ?\\nresponse: Follow the discord link : \\n\\n https://discord.com/channels/1090613684163850280/1114113976616353832/1114121330925768754', metadata={'source': 'How do I update source in power query ?', 'row': 73}),\n",
       " Document(page_content='prompt: How do I enable Power Pivot before using it for the first time ?\\nresponse: Follow the process in the link : \\n\\nhttps://drive.google.com/file/d/1-mO-v52h-YTY1s-v30liBJPu6Yj4OUxb/view?usp=share_link', metadata={'source': 'How do I enable Power Pivot before using it for the first time ?', 'row': 74})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b2b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for langchain embeddingn to find num.of embeddings\n",
    "\n",
    "#from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c16dbdd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install InstructorEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb559cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9db4d43e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --user sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4bda49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf1ad960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae5b94e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\saike\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\saike\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#this libarary was installed as it was requested by the next output\n",
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3ae17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to the video from codebasic we have to import HuggingFaceInstructurEmbedding\n",
    "#but here we are going with HUggingFAceEmbedding as we ran into error\n",
    "#may be due to changes made in the new version than the version followed in the vide\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "#importing FAISS as we decided to use it as our vectordatabase\n",
    "\n",
    "inst_embeddings = HuggingFaceEmbeddings()\n",
    "vectordb = FAISS.from_documents(documents=data ,embedding=inst_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79997419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6532c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(score_threshold = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4d39ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.', metadata={'source': 'Do you provide any job assistance?', 'row': 11}),\n",
       " Document(page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}),\n",
       " Document(page_content='prompt: How can I get help if I have a doubt and need support?\\nresponse: We have an active discord server where you can post your question and most of the time you will get an answer in a reasonable time frame.', metadata={'source': 'How can I get help if I have a doubt and need support?', 'row': 23}),\n",
       " Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdocs = retriever.invoke(\"how about job placement support?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9766b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectordb = Chroma.from_documents(data,\n",
    "#                            embedding=google_palm_embeddings,\n",
    "#                            persist_directory='./chromadb')\n",
    "# vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394d207",
   "metadata": {},
   "source": [
    "Create RetrievalQA chain along with prompt template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfd94f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\" Given the following context and a question,generate an answer based on the this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source docuemnt context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know you can reach out to an agent.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "# we will use this RetrievalQA class of LangChain\n",
    "#to pull the similar looking embedding vectors to form a prompt\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                   chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    input_key=\"query\",\n",
    "                                    return_source_documents=True, # can be changed to false if you dont want to see the source documents from where its referring from\n",
    "                                    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "938fedde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you provide job assistance and also do you provide job gurantee?',\n",
       " 'result': 'Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.\\n\\nHowever, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.',\n",
       " 'source_documents': [Document(page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.', metadata={'source': 'Do you provide any job assistance?', 'row': 11}),\n",
       "  Document(page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this course guarantee me a job?', 'row': 33}),\n",
       "  Document(page_content='prompt: Will this bootcamp guarantee me a job?\\nresponse: The courses included in this bootcamp are done by 9000+ learners and many of them have secured a job which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this bootcamp guarantee me a job?', 'row': 15}),\n",
       "  Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14})]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain('Do you provide job assistance and also do you provide job gurantee?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4f11538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you guys provide internship and also do you offer EMI payments?',\n",
       " 'result': \"I don't know you can reach out to an agent.\",\n",
       " 'source_documents': [Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}),\n",
       "  Document(page_content='prompt: Do we have an EMI option?\\nresponse: No', metadata={'source': 'Do we have an EMI option?', 'row': 13}),\n",
       "  Document(page_content='prompt: Do you provide any job assistance?\\nresponse: Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.', metadata={'source': 'Do you provide any job assistance?', 'row': 11}),\n",
       "  Document(page_content='prompt: Will this course guarantee me a job?\\nresponse: We created a much lighter version of this course on YouTube available for free (click this link) and many people gave us feedback that they were able to fetch jobs (see testimonials). Now this paid course is at least 5x better than the YouTube course which gives us ample confidence that you will be able to get a job. However, we want to be honest and do not want to make any impractical promises! Our guarantee is to prepare you for the job market by teaching the most relevant skills, knowledge & timeless principles good enough to fetch the job.', metadata={'source': 'Will this course guarantee me a job?', 'row': 33})]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Do you guys provide internship and also do you offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d0981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
